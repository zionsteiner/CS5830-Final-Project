{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEC Data Collection\n",
    "### Steps\n",
    "1. Get urls for filings\n",
    "2. Get company info and filter by marketcap\n",
    "4. Download filings\n",
    "5. Clean filings\n",
    "6. Diff filings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Get urls for 2019 Q3 filings and more recent\n",
    "\n",
    "import edgar\n",
    "import os\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(), 'data/sec')\n",
    "# os.mkdir(data_dir)\n",
    "indices_dir = os.path.join(data_dir, 'indices')\n",
    "# os.mkdir(indices_dir)\n",
    "# edgar.download_index(indices_dir, 2019, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2. Get company info and filter by marketcap\n",
    "    a. Get list of ticker symbols\n",
    "    b. Join symbols with filings dataset on cik\n",
    "    c. Get marketcap for each\n",
    "    d. Get sector for each\n",
    "    e. Filter by marketcap\n",
    "\"\"\" \n",
    "# Reference code: https://github.com/PlatorSolutions/quarterly-earnings-machine-learning-algo/blob/master/download_raw_html.py\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "filings_dir = os.path.join(data_dir, 'filings')\n",
    "# os.mkdir(filings_dir)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for tsv in os.listdir(indices_dir):\n",
    "    if '.ipynb_checkpoints' in tsv:\n",
    "        continue\n",
    "    path = os.path.join(indices_dir, tsv)\n",
    "    df_ext = pd.read_csv(path, delimiter='|', names=['cik', 'name', 'type', 'date', 'text_uri', 'index_url'])\n",
    "    df = df.append(df_ext)\n",
    "    \n",
    "df_10Q = df[df['type'] == '10-Q'].reset_index()\n",
    "df_10Q['date'] = pd.to_datetime(df_10Q['date'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "def cik_to_ticker(cik):\n",
    "    endpoint = f'https://mapping-api.herokuapp.com/cik/{cik}'\n",
    "    res = requests.get(endpoint)\n",
    "    if len(res.json()) < 1:\n",
    "        return np.nan\n",
    "    return res.json()[0]['ticker']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ticker_to_company_info(ticker):\n",
    "    key_id = 'ALPACA or POLYGON key'\n",
    "    endpoint = f'https://api.polygon.io/v1/meta/symbols/{ticker}/company?apiKey={key_id}'\n",
    "    res = requests.get(endpoint)\n",
    "    if res.status_code == 200 and 'json' in res.headers['Content-Type']:\n",
    "        return res.json()\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_10Q['ticker'] = df_10Q['cik'].apply(lambda cik: cik_to_ticker(cik))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_10Q = df_10Q.dropna()\n",
    "company_info_list = df_10Q['ticker'].apply(lambda ticker: ticker_to_company_info(ticker))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_index = company_info_list.notna()\n",
    "company_info_list = company_info_list.loc[bool_index]\n",
    "df_10Q = df_10Q.loc[bool_index].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_10Q['marketcap'] = company_info_list.apply(lambda info: info['marketcap'])\n",
    "df_10Q['sector'] = company_info_list.apply(lambda info: info['sector'])\n",
    "df_10Q = df_10Q.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_10Q['marketcap'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_10Q = df_10Q.loc[df_10Q['marketcap'] >= 1e9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.09% downloaded\n",
      "2.19% downloaded\n",
      "3.28% downloaded\n",
      "4.38% downloaded\n",
      "5.47% downloaded\n",
      "6.57% downloaded\n",
      "7.66% downloaded\n",
      "8.76% downloaded\n",
      "9.85% downloaded\n",
      "10.94% downloaded\n",
      "12.04% downloaded\n",
      "13.13% downloaded\n",
      "14.23% downloaded\n",
      "15.32% downloaded\n",
      "16.42% downloaded\n",
      "17.51% downloaded\n",
      "18.60% downloaded\n",
      "19.70% downloaded\n",
      "20.79% downloaded\n",
      "21.89% downloaded\n",
      "22.98% downloaded\n",
      "24.08% downloaded\n",
      "25.17% downloaded\n",
      "26.27% downloaded\n",
      "27.36% downloaded\n",
      "28.45% downloaded\n",
      "29.55% downloaded\n",
      "30.64% downloaded\n",
      "31.74% downloaded\n",
      "32.83% downloaded\n",
      "33.93% downloaded\n",
      "35.02% downloaded\n",
      "36.11% downloaded\n",
      "37.21% downloaded\n",
      "38.30% downloaded\n",
      "39.40% downloaded\n",
      "40.49% downloaded\n",
      "41.59% downloaded\n",
      "42.68% downloaded\n",
      "43.78% downloaded\n",
      "44.87% downloaded\n",
      "45.96% downloaded\n",
      "47.06% downloaded\n",
      "48.15% downloaded\n",
      "49.25% downloaded\n",
      "50.34% downloaded\n",
      "51.44% downloaded\n",
      "52.53% downloaded\n",
      "53.63% downloaded\n",
      "54.72% downloaded\n",
      "55.81% downloaded\n",
      "56.91% downloaded\n",
      "58.00% downloaded\n",
      "59.10% downloaded\n",
      "60.19% downloaded\n",
      "61.29% downloaded\n",
      "62.38% downloaded\n",
      "63.47% downloaded\n",
      "64.57% downloaded\n",
      "65.66% downloaded\n",
      "66.76% downloaded\n",
      "67.85% downloaded\n",
      "68.95% downloaded\n",
      "70.04% downloaded\n",
      "71.14% downloaded\n",
      "72.23% downloaded\n",
      "73.32% downloaded\n",
      "74.42% downloaded\n",
      "75.51% downloaded\n",
      "76.61% downloaded\n",
      "77.70% downloaded\n",
      "78.80% downloaded\n",
      "79.89% downloaded\n",
      "80.98% downloaded\n",
      "82.08% downloaded\n",
      "83.17% downloaded\n",
      "84.27% downloaded\n",
      "85.36% downloaded\n",
      "86.46% downloaded\n",
      "87.55% downloaded\n",
      "88.65% downloaded\n",
      "89.74% downloaded\n",
      "90.83% downloaded\n",
      "91.93% downloaded\n",
      "93.02% downloaded\n",
      "94.12% downloaded\n",
      "95.21% downloaded\n",
      "96.31% downloaded\n",
      "97.40% downloaded\n",
      "98.50% downloaded\n",
      "99.59% downloaded\n"
     ]
    }
   ],
   "source": [
    "# 3. Download filings\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "filing_dir = os.path.join(data_dir, 'raw_filings')\n",
    "# os.mkdir(filing_dir)\n",
    "\n",
    "sec_url = 'https://www.sec.gov/Archives'\n",
    "min_change = 1\n",
    "curr_change = 0\n",
    "path_list = []\n",
    "for i, filing in df_10Q.iterrows():\n",
    "    if i > 0:\n",
    "        curr_val = (i / len(df_10Q)) * 100\n",
    "        curr_change += curr_val - (((i - 1) / len(df_10Q)) * 100)\n",
    "        if curr_change >= min_change:\n",
    "            print(f'{curr_val:.2f}% downloaded')\n",
    "            curr_change = 0\n",
    "    \n",
    "    # Get page\n",
    "    path = os.path.join(sec_url, filing['index_url'])\n",
    "    res = requests.get(path)\n",
    "    soup = BeautifulSoup(res.content, 'lxml')\n",
    "    \n",
    "    # Find download href\n",
    "    files_table = soup.find('table', {'class': 'tableFile', 'summary': 'Document Format Files'})\n",
    "    anchor_element = None\n",
    "    for row in files_table.find_all('tr'):\n",
    "        e = row.find_all('td')\n",
    "        if len(e) >= 4 and e[3].text == filing['type']:\n",
    "            anchor_element = e[2].find('a', href=True)\n",
    "            break\n",
    "    \n",
    "    anchor = anchor_element['href']\n",
    "    name = anchor_element.text\n",
    "    acceptance_date = soup.find('div', attrs={'class': 'infoHead'}, text='Accepted').findNext('div', {'class': 'info'}).text\n",
    "\n",
    "    if 'ix?' in anchor:\n",
    "        anchor = '/' + '/'.join(anchor.split('/')[2:])\n",
    "    \n",
    "    # Download\n",
    "    sec_base = 'https://www.sec.gov'\n",
    "    download_res = requests.get(sec_base + anchor)\n",
    "    \n",
    "    # Save\n",
    "    file_name = '_'.join([filing['name'].replace(' ', '').replace('/', ''), filing['type'], filing['date'].strftime('%Y-%m')]) + '.htm'\n",
    "    file_path = os.path.join(filing_dir, file_name)\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(download_res.text)\n",
    "    \n",
    "    path_list.append(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_10Q['path'] = path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_10Q.to_csv(os.path.join(data_dir, 'df_10Q.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_10Q = pd.read_csv(os.path.join(data_dir, 'df_10Q.csv'), parse_dates=['date']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Clean filings\n",
    "import html\n",
    "import multiprocessing as mp\n",
    "import re\n",
    "from functools import partial\n",
    "\n",
    "def removeInnerLinks(soup):\n",
    "    [i.extract() for i in soup.find_all('a', href=True) if len(i['href']) > 0 and i['href'][0] == '#']\n",
    "    return soup\n",
    "\n",
    "def remove_xbrli(soup):\n",
    "    [x.extract() for x in soup.find_all(re.compile(\"^xbrli:\"))]\n",
    "    return soup\n",
    "\n",
    "def removeNumericalTables(soup):\n",
    "\n",
    "    def GetDigitPercentage(tablestring):\n",
    "        if len(tablestring) > 0.0:\n",
    "            numbers = sum([char.isdigit() for char in tablestring])\n",
    "            length = len(tablestring)\n",
    "            return numbers / length\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    def containsBgColor(table):\n",
    "        for row in table.find_all('tr'):\n",
    "            colored = 'background-color' in str(row) or 'bgcolor' in str(row)\n",
    "            if colored:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    [x.extract() for x in soup.find_all('table') if containsBgColor(x)]\n",
    "\n",
    "    [x.extract() for x in soup.find_all('table') if GetDigitPercentage(x.get_text()) > 0.15]\n",
    "\n",
    "    return soup\n",
    "\n",
    "def prep_text(text):\n",
    "    soup = BeautifulSoup(html.unescape(re.sub(r'\\s+', ' ', text)), \"lxml\")\n",
    "\n",
    "    soup = remove_xbrli(soup)\n",
    "\n",
    "    soup = removeInnerLinks(soup)\n",
    "\n",
    "    soup = removeNumericalTables(soup)\n",
    "\n",
    "    [x.unwrap() for x in soup.find_all(['span', 'font', 'b', 'i', 'u', 'strong'])]\n",
    "\n",
    "    soup.smooth()\n",
    "\n",
    "    text = soup.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "    stop_words = set(['HUNDRED', 'THOUSAND', 'MILLION', 'BILLION', 'TRILLION', 'DATE', 'ANNUAL', 'ANNUALLY', 'ANNUM', 'YEAR', 'YEARLY', 'QUARTER', 'QUARTERLY', 'QTR', 'MONTH', 'MONTHLY', 'WEEK', 'WEEKLY', 'DAY', 'DAILY', 'JANUARY', 'FEBRUARY', 'MARCH', 'APRIL', 'MAY', 'JUNE', 'JULY', 'AUGUST', 'SEPTEMBER', 'OCTOBER', 'NOVEMBER', 'DECEMBER', 'JAN', 'FEB', 'MAR', 'APR', 'MAY', 'JUN', 'JUL', 'AUG', 'SEP', 'SEPT', 'OCT', 'NOV', 'DEC', 'MONDAY', 'TUESDAY', 'WEDNESDAY', 'THURSDAY', 'FRIDAY', 'SATURDAY', 'SUNDAY', 'ONE', 'TWO', 'THREE', 'FOUR', 'FIVE', 'SIX', 'SEVEN', 'EIGHT', 'NINE', 'TEN', 'ELEVEN', 'TWELVE', 'THIRTEEN', 'FOURTEEN', 'FIFTEEN', 'SIXTEEN', 'SEVENTEEN', 'EIGHTEEN', 'NINETEEN', 'TWENTY', 'THIRTY', 'FORTY', 'FIFTY', 'SIXTY', 'SEVENTY', 'EIGHTY', 'NINETY', 'FIRST', 'SECOND', 'THIRD', 'FOURTH', 'FIFTH', 'SIXTH', 'SEVENTH', 'EIGHTH', 'NINTH', 'TENTH'])\n",
    "    pattern = re.compile(r'\\b(' + r'|'.join(stop_words) + r')\\b\\s*', re.IGNORECASE)\n",
    "    text = pattern.sub('', text)\n",
    "\n",
    "    pattern = re.compile('\\s[^a-zA-Z\\s]+?(?=(\\.*\\s))')\n",
    "    text = pattern.sub('', text)\n",
    "\n",
    "    text = '\\n'.join(\n",
    "        filter(lambda line: len(line) > 0 and (sum(i.isalpha() for i in line) / len(line) > .5), text.splitlines()))\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def prep_file(path):\n",
    "    with open(path) as f:\n",
    "        text = prep_text(f.read())\n",
    "    return text\n",
    "\n",
    "def saveBag(c, old_file_name):\n",
    "    file_name = f'cleaned_{old_file_name.replace(\".htm\", \".txt\")}'\n",
    "    file_path = os.path.join(cleaned_filings_dir, file_name)\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:  [Errno 21] Is a directory: '/home/zion/Documents/cs5830/final_project/data/sec/raw_filings/.ipynb_checkpoints'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "cleaned_filings_dir = os.path.join(data_dir, 'cleaned_filings')\n",
    "# os.mkdir(cleaned_filings_dir)\n",
    "\n",
    "pool = mp.Pool(mp.cpu_count())\n",
    "for file in os.listdir(filing_dir):\n",
    "    file_path = os.path.join(filing_dir, file)\n",
    "    \n",
    "    callback = partial(saveBag, old_file_name=file)\n",
    "    pool.apply_async(prep_file, args=[file_path], callback=callback, error_callback=lambda x: print('Error: ', x))\n",
    "    \n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text(path):\n",
    "    with open(path, 'r') as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_10Q['text'] = df_10Q['path'].apply(lambda path: load_text(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove some missed garbage tokens\n",
    "from unidecode import unidecode\n",
    "df_10Q['text'] = df_10Q['text'].apply(lambda text: unidecode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compress\n",
    "df_10Q.to_pickle('df_10Q.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ToDo:\n",
    "* possibly divide filings by section\n",
    "* experiment with diffing filings (fuzzywuzzy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
