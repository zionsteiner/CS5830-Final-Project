{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEC Data Collection\n",
    "### Steps\n",
    "1. Get urls for filings\n",
    "2. Get company info and filter by marketcap\n",
    "4. Download filings\n",
    "5. Clean filings\n",
    "6. Diff filings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Get urls for 2019 Q3 filings and more recent\n",
    "\n",
    "import edgar\n",
    "import os\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(), 'data/sec')\n",
    "# os.mkdir(data_dir)\n",
    "indices_dir = os.path.join(data_dir, 'indices')\n",
    "# os.mkdir(indices_dir)\n",
    "# edgar.download_index(indices_dir, 2019, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2. Get company info and filter by marketcap\n",
    "    a. Get list of ticker symbols\n",
    "    b. Join symbols with filings dataset on cik\n",
    "    c. Get marketcap for each\n",
    "    d. Get sector for each\n",
    "    e. Filter by marketcap\n",
    "\"\"\" \n",
    "# Reference code: https://github.com/PlatorSolutions/quarterly-earnings-machine-learning-algo/blob/master/download_raw_html.py\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "filings_dir = os.path.join(data_dir, 'filings')\n",
    "# os.mkdir(filings_dir)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for tsv in os.listdir(indices_dir):\n",
    "    if '.ipynb_checkpoints' in tsv:\n",
    "        continue\n",
    "    path = os.path.join(indices_dir, tsv)\n",
    "    df_ext = pd.read_csv(path, delimiter='|', names=['cik', 'name', 'type', 'date', 'text_uri', 'index_url'])\n",
    "    df = df.append(df_ext)\n",
    "    \n",
    "df_10Q = df[df['type'] == '10-Q'].reset_index()\n",
    "df_10Q['date'] = pd.to_datetime(df_10Q['date'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "def cik_to_ticker(cik):\n",
    "    endpoint = f'https://mapping-api.herokuapp.com/cik/{cik}'\n",
    "    res = requests.get(endpoint)\n",
    "    if len(res.json()) < 1:\n",
    "        return np.nan\n",
    "    return res.json()[0]['ticker']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ticker_to_company_info(ticker):\n",
    "    key_id = 'AKQLE29QN0JRVM0Y786Y'\n",
    "    endpoint = f'https://api.polygon.io/v1/meta/symbols/{ticker}/company?apiKey={key_id}'\n",
    "    res = requests.get(endpoint)\n",
    "    if res.status_code == 200 and 'json' in res.headers['Content-Type']:\n",
    "        return res.json()\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_10Q['ticker'] = df_10Q['cik'].apply(lambda cik: cik_to_ticker(cik))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "df_10Q = df_10Q.dropna()\n",
    "company_info_list = df_10Q['ticker'].apply(lambda ticker: ticker_to_company_info(ticker))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_index = company_info_list.notna()\n",
    "company_info_list = company_info_list.loc[bool_index]\n",
    "df_10Q = df_10Q.loc[bool_index].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_10Q['marketcap'] = company_info_list.apply(lambda info: info['marketcap'])\n",
    "df_10Q['sector'] = company_info_list.apply(lambda info: info['sector'])\n",
    "df_10Q = df_10Q.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_10Q['marketcap'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_10Q = df_10Q.loc[df_10Q['marketcap'] >= 1e9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_10Q = df_10Q.reset_index(drop=True).drop(['level_0', 'index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Download filings\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "filing_dir = os.path.join(data_dir, 'raw_filings')\n",
    "# os.mkdir(filing_dir)\n",
    "\n",
    "sec_url = 'https://www.sec.gov/Archives'\n",
    "min_change = 1\n",
    "curr_change = 0\n",
    "path_list = []\n",
    "for i, filing in df_10Q.iterrows():\n",
    "    if i > 0:\n",
    "        curr_val = (i / len(df_10Q)) * 100\n",
    "        curr_change += curr_val - (((i - 1) / len(df_10Q)) * 100)\n",
    "        if curr_change >= min_change:\n",
    "            print(f'{curr_val:.2f}% downloaded')\n",
    "            curr_change = 0\n",
    "    \n",
    "    # Get page\n",
    "    path = os.path.join(sec_url, filing['index_url'])\n",
    "    res = requests.get(path)\n",
    "    soup = BeautifulSoup(res.content, 'lxml')\n",
    "    \n",
    "    # Find download href\n",
    "    files_table = soup.find('table', {'class': 'tableFile', 'summary': 'Document Format Files'})\n",
    "    anchor_element = None\n",
    "    for row in files_table.find_all('tr'):\n",
    "        e = row.find_all('td')\n",
    "        if len(e) >= 4 and e[3].text == filing['type']:\n",
    "            anchor_element = e[2].find('a', href=True)\n",
    "            break\n",
    "    \n",
    "    anchor = anchor_element['href']\n",
    "    name = anchor_element.text\n",
    "    acceptance_date = soup.find('div', attrs={'class': 'infoHead'}, text='Accepted').findNext('div', {'class': 'info'}).text\n",
    "\n",
    "    if 'ix?' in anchor:\n",
    "        anchor = '/' + '/'.join(anchor.split('/')[2:])\n",
    "    \n",
    "    # Download\n",
    "    sec_base = 'https://www.sec.gov'\n",
    "    download_res = requests.get(sec_base + anchor)\n",
    "    \n",
    "    # Save\n",
    "    file_name = '_'.join([filing['name'].replace(' ', '').replace('/', ''), filing['type'], filing['date'].strftime('%Y-%m')]) + '.htm'\n",
    "    file_path = os.path.join(filing_dir, file_name)\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(download_res.text)\n",
    "    \n",
    "    path_list.append(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_10Q['path'] = path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Clean filings\n",
    "import html\n",
    "import multiprocessing as mp\n",
    "import re\n",
    "from functools import partial\n",
    "\n",
    "def removeInnerLinks(soup):\n",
    "    [i.extract() for i in soup.find_all('a', href=True) if len(i['href']) > 0 and i['href'][0] == '#']\n",
    "    return soup\n",
    "\n",
    "def remove_xbrli(soup):\n",
    "    [x.extract() for x in soup.find_all(re.compile(\"^xbrli:\"))]\n",
    "    return soup\n",
    "\n",
    "def removeNumericalTables(soup):\n",
    "\n",
    "    def GetDigitPercentage(tablestring):\n",
    "        if len(tablestring) > 0.0:\n",
    "            numbers = sum([char.isdigit() for char in tablestring])\n",
    "            length = len(tablestring)\n",
    "            return numbers / length\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    def containsBgColor(table):\n",
    "        for row in table.find_all('tr'):\n",
    "            colored = 'background-color' in str(row) or 'bgcolor' in str(row)\n",
    "            if colored:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    [x.extract() for x in soup.find_all('table') if containsBgColor(x)]\n",
    "\n",
    "    [x.extract() for x in soup.find_all('table') if GetDigitPercentage(x.get_text()) > 0.15]\n",
    "\n",
    "    return soup\n",
    "\n",
    "def prep_text(text):\n",
    "    soup = BeautifulSoup(html.unescape(re.sub(r'\\s+', ' ', text)), \"lxml\")\n",
    "\n",
    "    soup = remove_xbrli(soup)\n",
    "\n",
    "    soup = removeInnerLinks(soup)\n",
    "\n",
    "    soup = removeNumericalTables(soup)\n",
    "\n",
    "    [x.unwrap() for x in soup.find_all(['span', 'font', 'b', 'i', 'u', 'strong'])]\n",
    "\n",
    "    soup.smooth()\n",
    "\n",
    "    text = soup.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "    stop_words = set(['HUNDRED', 'THOUSAND', 'MILLION', 'BILLION', 'TRILLION', 'DATE', 'ANNUAL', 'ANNUALLY', 'ANNUM', 'YEAR', 'YEARLY', 'QUARTER', 'QUARTERLY', 'QTR', 'MONTH', 'MONTHLY', 'WEEK', 'WEEKLY', 'DAY', 'DAILY', 'JANUARY', 'FEBRUARY', 'MARCH', 'APRIL', 'MAY', 'JUNE', 'JULY', 'AUGUST', 'SEPTEMBER', 'OCTOBER', 'NOVEMBER', 'DECEMBER', 'JAN', 'FEB', 'MAR', 'APR', 'MAY', 'JUN', 'JUL', 'AUG', 'SEP', 'SEPT', 'OCT', 'NOV', 'DEC', 'MONDAY', 'TUESDAY', 'WEDNESDAY', 'THURSDAY', 'FRIDAY', 'SATURDAY', 'SUNDAY', 'ONE', 'TWO', 'THREE', 'FOUR', 'FIVE', 'SIX', 'SEVEN', 'EIGHT', 'NINE', 'TEN', 'ELEVEN', 'TWELVE', 'THIRTEEN', 'FOURTEEN', 'FIFTEEN', 'SIXTEEN', 'SEVENTEEN', 'EIGHTEEN', 'NINETEEN', 'TWENTY', 'THIRTY', 'FORTY', 'FIFTY', 'SIXTY', 'SEVENTY', 'EIGHTY', 'NINETY', 'FIRST', 'SECOND', 'THIRD', 'FOURTH', 'FIFTH', 'SIXTH', 'SEVENTH', 'EIGHTH', 'NINTH', 'TENTH'])\n",
    "    pattern = re.compile(r'\\b(' + r'|'.join(stop_words) + r')\\b\\s*', re.IGNORECASE)\n",
    "    text = pattern.sub('', text)\n",
    "\n",
    "    pattern = re.compile('\\s[^a-zA-Z\\s]+?(?=(\\.*\\s))')\n",
    "    text = pattern.sub('', text)\n",
    "\n",
    "    text = '\\n'.join(\n",
    "        filter(lambda line: len(line) > 0 and (sum(i.isalpha() for i in line) / len(line) > .5), text.splitlines()))\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def prep_file(path):\n",
    "    with open(path) as f:\n",
    "        text = prep_text(f.read())\n",
    "    return text\n",
    "\n",
    "def saveBag(c, old_file_name):\n",
    "    file_name = f'cleaned_{old_file_name.replace(\".htm\", \".txt\")}'\n",
    "    file_path = os.path.join(cleaned_filings_dir, file_name)\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "cleaned_filings_dir = os.path.join(data_dir, 'cleaned_filings')\n",
    "# os.mkdir(cleaned_filings_dir)\n",
    "\n",
    "pool = mp.Pool(4)\n",
    "for file in os.listdir(filing_dir):\n",
    "    file_path = os.path.join(filing_dir, file)\n",
    "    \n",
    "    callback = partial(saveBag, old_file_name=file)\n",
    "    pool.apply_async(prep_file, args=[file_path], callback=callback, error_callback=lambda x: print('Error: ', x))\n",
    "    \n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text(path):\n",
    "    with open(path, 'r') as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change path to match cleaned_filings path instead of raw_filings\n",
    "df_10Q['path'] = df_10Q['path'].str.replace('raw_filings/', 'cleaned_filings/cleaned_')\n",
    "df_10Q['path'] = df_10Q['path'].str.replace('.htm', '.txt')\n",
    "df_10Q['text'] = df_10Q['path'].apply(lambda path: load_text(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove some missed garbage tokens\n",
    "from unidecode import unidecode\n",
    "df_10Q['text'] = df_10Q['text'].apply(lambda text: unidecode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compress\n",
    "df_10Q.to_pickle(os.path.join(data_dir, 'df_10Q.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ToDo:\n",
    "* possibly divide filings by section\n",
    "* experiment with diffing filings (fuzzywuzzy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "df_10Q = pd.read_pickle(os.path.join(data_dir, 'df_10Q.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "diff_dir = os.path.join(data_dir, 'diffed_filings')\n",
    "# os.mkdir(diff_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "import spacy\n",
    "from fuzzywuzzy import process, fuzz\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def diff_text(text1, text2):\n",
    "    new_sentences = []\n",
    "    \n",
    "    # Load into spacy\n",
    "    doc1 = nlp(text1)\n",
    "    doc1_sents = list(map(lambda sent: sent.text, list(doc1.sents)))\n",
    "    doc2 = nlp(text2)\n",
    "    doc2_sents = list(map(lambda sent: sent.text, list(doc2.sents)))\n",
    "    \n",
    "    # Take diff\n",
    "    for sent in doc1_sents:\n",
    "        match = process.extractOne(sent, doc2_sents, score_cutoff=85, scorer=fuzz.QRatio)\n",
    "        if not match:\n",
    "            new_sentences.append(sent)\n",
    "    \n",
    "    return '\\n'.join(new_sentences)\n",
    "\n",
    "def save_diff(diff, name):\n",
    "    path = os.path.join(diff_dir, name)\n",
    "    with open(path, 'w') as file:\n",
    "        print('saving...')\n",
    "        file.write(diff)\n",
    "\n",
    "pool = mp.Pool(4)\n",
    "for cik in df_10Q['cik'].unique():\n",
    "    filings = df_10Q.loc[df_10Q['cik'] == cik].sort_values('date', ascending=True).reset_index()\n",
    "    for i, filing in filings.iterrows():\n",
    "        if (i + 1) < len(filings):\n",
    "            name = f'{cik}:{filing[\"date\"].strftime(\"%m-%d-%Y\")}.txt'\n",
    "            if name in os.listdir(diff_dir):\n",
    "                continue\n",
    "            prev_filing = df_10Q.iloc[i+1]\n",
    "            \n",
    "            callback = partial(save_diff, name=name)\n",
    "            pool.apply_async(diff_text, args=(filing['text'], prev_filing['text']), callback=callback, error_callback=lambda x: print('Error: ', x))\n",
    "            \n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/sec/cleaned_filings/cleaned_ABERCROMBIE&FITCHCODE_10-Q_2019-12.txt') as file:\n",
    "    t = file.read()\n",
    "    d = diff_text(t, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df = pd.DataFrame(columns=['cik', 'date', 'diff'])\n",
    "for filename in os.listdir(diff_dir):\n",
    "    try:\n",
    "        cik, date = filename.split('.')[0].split(':')\n",
    "    except Exception as e:\n",
    "        print(filename)\n",
    "        continue\n",
    "    \n",
    "    file_path = os.path.join(diff_dir, filename)\n",
    "    with open(file_path) as file:\n",
    "        diff = file.read()\n",
    "    diff_df = diff_df.append({'cik': cik, 'date': date, 'diff': diff}, ignore_index=True)\n",
    "    \n",
    "diff_df['date'] = pd.to_datetime(diff_df['date'], format='%m-%d-%Y')\n",
    "df_10Q = pd.concat([df_10Q, diff_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_10Q.to_pickle(os.path.join(data_dir, 'df_10Q.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "df_10Q = pd.read_pickle(os.path.join(data_dir, 'df_10Q.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bars and Earnings Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
